#=
This file is required by vcf_classifier.
It can not only load normal vcf file, but can also load 'db-vcf' file, which
is generated by the workflow of vcf_classifer.
=#

# hot fix when Base.stdout is changed, readchomp will capture nothing.
function Base.readchomp(x::Base.AbstractCmd)
    io = IOBuffer()
    run(pipeline(x, stdout=io))
    res = chomp(String(take!(io)))
    close(io)
    res
end

function vcf_load(vcf_path::AbstractString)

    isfile(vcf_path) || error("VCF Load Error: File not exist: $vcf_path")

    header_string = ""
    vcf_contents = Vector{UInt8}()

    for line in eachline(vcf_path; keep=true)
        length(line) < 3 && continue
        line[1] == '#' && line[2] == '#' && continue
        if occursin(r"^([^\t]+\t)?#CHROM", line)
            # header line
            if isempty(header_string)
                header_string = replace(line[1:end-1], "#" => "", count=1)
            end
        else
            append!(vcf_contents, line)
        end
    end

    if length(vcf_contents) == 0
        @error Pipelines.timestamp() * "VCF Load Error: VCF data rows are empty: $vcf_path"
        return nothing
    end

    header = split(header_string, "\t")
    header = String.(header)

    # check
    for i in ["CHROM", "POS", "REF", "ALT", "FORMAT"]
        if !(i in header)
            # close(io)
            error("VCF Load Error:$i should be a header of vcf file $vcf_path")
        end
    end

    # change the header name after FORMAT
    idx_FORMAT = findfirst(x -> x == "FORMAT", header)
    if length(header) < idx_FORMAT + 1
        error("VCF Load Error: no column after the column named FORMAT.")
    end
    header[idx_FORMAT + 1] = "STAT"

    # if db-vcf, the first column is not CHROM, is SAMPLE
    if findfirst(x -> x == "CHROM", header) == 2
        header[1] = "SAMPLE"
    end

    # vcf column types
    col_types = Dict(
        :POS => Int64,
        :QUAL => Float64
    )

    # load vcf
    df = CSV.read(vcf_contents, DataFrame, header=header, types=col_types, delim='\t', ntasks=1)

    ### remove rows without proper FORMAT
    all_formats = countmap(df.FORMAT)
    # FORMAT must contain AD
    filter!(x -> occursin(r"(^|:)AD(:|$)", x.first), all_formats)

    if isempty(all_formats)
        @error Pipelines.timestamp() * "VCF Load Error: VCF data rows do not contain AD: it implies no data was mapped to reference: $vcf_path"
        # close(io)
        return nothing
    end

    # use prevalent one
    format_max_occurance = maximum(values(all_formats))
    filter!(x -> x.second == format_max_occurance, all_formats)
    df_format_string = collect(keys(all_formats))[1]
    # filter other rows
    df = df[df.FORMAT .== df_format_string, :]

    ### remove rows STAT == "."
    df = df[df.STAT .!= ".", :]

    df_format = split(df_format_string, ":")
    df_nformat = length(df_format)
    df_stat = DataFrame(reshape(vcat(split.(df.STAT, ':')...), df_nformat, :) |> permutedims, Symbol.(df_format))
    df_stat_eltypes = [String, Int, String, Int, Int, Int, Int]
    for i in 1:df_nformat
        df[!, Symbol(df_format[i])] = df_stat[!,i]
    end

    # delete cols
    select!(df, Not([:INFO, :FORMAT, :STAT]))

    # column SAMPLE only can be found in --db-vcf. It is required!
    # the format is GROUP_ID/SAMPLE_ID, such as HA/GCA_000756225.1_ASM75622v1
    if :SAMPLE in propertynames(df)
        df[:, :GROUP] = [split(x, "/")[1] for x in df.SAMPLE]
    end

    # close(io)
    df
end

function generate_db_vcf(db_vcf_path::AbstractString, inputs::Vector, labels::Vector)
    rm(db_vcf_path, force=true)
    rm(db_vcf_path * ".tmp", force=true)
    io_db_vcf = open(db_vcf_path * ".tmp" , "w+")
    io_header = open(db_vcf_path * ".header" , "w+")

    for (index, input) in enumerate(inputs)
        label = labels[index]
        for line in eachline(input)
            length(line) < 2 && continue

            # handle header lines
            if line[1] == '#'
                if line[2] == '#' # description line: ##...
                    index == 1 && println(io_db_vcf, line)
                    continue
                end
                if line[2] == 'C'  # header line: #CHROM  POS...
                    # save to header files
                    # additionally, write to $db_vcf_path.header
                    print(io_header, label, "\t")
                    println(io_header, line)
                    index > 1 && continue
                    # for compatibility, still write to io_db_vcf
                end
            end
            # normal lines
            print(io_db_vcf, label, "\t")
            println(io_db_vcf, line)
        end
    end
    close(io_db_vcf)
    close(io_header)
    run(`sort --parallel=$(Sys.CPU_THREADS ÷ 2 > 8 ? 8 : Sys.CPU_THREADS ÷ 2 + 1) -V -k2 -k3 -k6 $db_vcf_path.tmp -o $db_vcf_path`)
    rm(db_vcf_path * ".tmp", force=true)
end

function get_sample_info_from_db_vcf(db_vcf_path::AbstractString)
    # GROUP/SAMPLE
    group_samples = if isfile(db_vcf_path * ".header")
        split(readchomp(`awk '$2=="#CHROM"{print $1}' $db_vcf_path.header`))
    else
        # compatibility for old version
        split(readchomp(`awk '$2=="#CHROM"{print $1}' $db_vcf_path`))
    end

    group_dict = Dict{SubString{String}, Vector{SubString{String}}}()
    nsample_group = Dict{SubString{String}, Int}()
    for group_sample in group_samples
        group, sample = split(group_sample, "/")
        if haskey(group_dict, group)
            push!(group_dict[group], sample)
            nsample_group[group] += 1
        else
            group_dict[group] = [sample]
            nsample_group[group] = 1
        end
    end
    group_dict, nsample_group
end

"""
    get_sample_info_from_labels(group_samples::Vector)

- Element format of `group_samples`: GROUP/SAMPLE

Return group_dict, nsample_group.

See also `get_sample_info_from_db_vcf(db_vcf_path::AbstractString)`.
"""
function get_sample_info_from_labels(group_samples::AbstractVector)
    # GROUP/SAMPLE
    group_dict = Dict{SubString{String}, Vector{SubString{String}}}()
    nsample_group = Dict{SubString{String}, Int}()
    for group_sample in unique(group_samples)
        group, sample = split(group_sample, "/")
        if haskey(group_dict, group)
            push!(group_dict[group], sample)
            nsample_group[group] += 1
        else
            group_dict[group] = [sample]
            nsample_group[group] = 1
        end
    end
    group_dict, nsample_group
end

function value_normalize!(dict::Dict)
    sum_value = sum(values(dict))
    for key in keys(dict)
        dict[key] /= sum_value
    end
end
function value_normalize!(vec::Vector{Float64})
    sum_value = sum(vec)
    vec ./= sum_value
end
function value_normalize(vec)
    sum_value = sum(vec)
    vec ./ sum_value
end

"""
    parse_group_db_vcf(db_vcf::DataFrame, nsample_group::Dict; missing_as_ref::Bool=true, min_prob::Float64 = 0.05)

# Arguments

- `missing_as_ref::Bool`: If `true`, missing samples at a loci are treated as no alternation. Else, missing samples are regarded as missing.

- `min_prob::Float`: For each group at each location, if the probability of the SNP is less tham `min_prob`, the SNP will be removed.
"""
function parse_group_db_vcf(db_vcf::DataFrame, nsample_group::Dict; missing_as_ref::Bool=true, min_prob::Float64 = 0.05)
    groups = collect(keys(nsample_group))
    nsamples = collect(values(nsample_group))
    ngroup = length(groups)
    # check
    for colname_check in [:SAMPLE, :CHROM, :POS, :REF, :ALT, :AD]
        colname_check in propertynames(db_vcf) || error("$colname_check should be a header of --db-vcf")
    end

    # filter rows: REF == "N"
    # filter rows: REF == "<*>", TODO: not sure what does that means in VCF format 4.2
    filter!(row -> row.REF != "N" && !ismissing(row.ALT), db_vcf)

    # db_vcf_parsed = by(db_vcf, [:CHROM, :POS]) do df
    gdf = groupby(db_vcf, [:CHROM, :POS])

    db_vcf_parsed = combine(gdf) do df
        ALT2PROBs = Dict{SubString{String}, Vector{Float64}}()
        for igroup in 1:ngroup
            group = groups[igroup]
            nsample = nsamples[igroup]

            sample_indices = findall(x -> x == group, df.GROUP)
            nsample_alter = length(sample_indices)  # num sample of this group at this locus

            if nsample_alter == 0
                if missing_as_ref
                    if haskey(ALT2PROBs, ".")
                        ALT2PROBs["."][igroup] += 1.0
                    else
                        ALT2PROBs["."] = zeros(ngroup)
                        ALT2PROBs["."][igroup] += 1.0
                    end
                end
            else
                ALT2PROB = Dict{SubString{String}, Float64}()
                for i_sample in sample_indices
                    row_ALTs = split(df[i_sample, :ALT], ',')

                    row_DEPTHs = parse.(Int, split(df[i_sample, :AD], ','))
                    row_PROBs = row_DEPTHs ./ sum(row_DEPTHs)

                    # diff in missing_as_ref: length of row_PROBs, row_ALTs are different (. does not count)
                    if length(row_ALTs) + 1 == length(row_PROBs)
                        # REF get some coverage: add REF (.) to ALT2PROB
                        if row_DEPTHs[1] > 0
                            if haskey(ALT2PROB, ".")
                                ALT2PROB["."] += row_PROBs[1]
                            else
                                ALT2PROB["."] = row_PROBs[1]
                            end
                        end

                        # Add ALT to ALT2PROB
                        for (idx_ALT, ALT) in enumerate(row_ALTs)
                            if haskey(ALT2PROB, ALT)
                                ALT2PROB[ALT] += row_PROBs[1+idx_ALT]
                            else
                                ALT2PROB[ALT] = row_PROBs[1+idx_ALT]
                            end
                        end
                    elseif length(row_ALTs) == length(row_PROBs)
                        # . in row_ALTs
                        for (idx_ALT, ALT) in enumerate(row_ALTs)
                            if haskey(ALT2PROB, ALT)
                                ALT2PROB[ALT] += row_PROBs[idx_ALT]
                            else
                                ALT2PROB[ALT] = row_PROBs[idx_ALT]
                            end
                        end
                    else
                        @error Pipelines.timestamp() * "Bugs here. Invalid row_ALTs and row_DEPTHs" row_ALTs row_DEPTHs group nsample sample_indices df
                    end
                end

                if missing_as_ref && nsample_alter < nsample
                    # missing sample as reference
                    if haskey(ALT2PROB, ".")
                        ALT2PROB["."] += (nsample - nsample_alter)
                    else
                        ALT2PROB["."] = (nsample - nsample_alter)
                    end
                end

                value_normalize!(ALT2PROB)

                # filter: remove low probability < min_prob
                if min_prob > 0.0
                    filter!(x -> x.second >= min_prob, ALT2PROB)
                    value_normalize!(ALT2PROB)
                end

                # copy items from ALT2PROB to ALT2PROBs
                for ALT in keys(ALT2PROB)
                    if haskey(ALT2PROBs, ALT)
                        ALT2PROBs[ALT][igroup] += ALT2PROB[ALT]
                    else
                        ALT2PROBs[ALT] = zeros(ngroup)
                        ALT2PROBs[ALT][igroup] += ALT2PROB[ALT]
                    end
                end
            end
        end
        (REF = df[1,:REF], ALT2PROBs = ALT2PROBs, )  # return of the function of by(...)
    end
end

function compute_input_probabilities(input_vcf_all::DataFrame, db_vcf_parsed::DataFrame, groups::Vector)

    # filter input vcf to db_vcf
    join_on = [:CHROM, :POS]
    :REF in propertynames(input_vcf_all) && push!(join_on, :REF)
    vcf = rightjoin(input_vcf_all, db_vcf_parsed, on=join_on)

    for colname_check in [:CHROM, :POS, :REF, :ALT, :AD]
        colname_check in propertynames(vcf) || error("$colname_check should be a header of --input")
    end

    # warn if data is not sufficient
    hasmissing(vcf.REF) &&
        @warn Pipelines.timestamp() * "--input does not contain all informative positions in --db-vcf"  count_all_position=nrow(db_vcf_parsed) count_missing=count(ismissing, vcf.REF)

    ngroup = length(groups)
    nrow_vcf = nrow(vcf)

    PCT_IN_DBs = Float64[]
    PROB_PUREs = Vector{Float64}[]
    PROBs = Vector{Float64}[]
    for irow in 1:nrow_vcf
        ALT2PROBs = vcf[irow, :ALT2PROBs]
        ALTs_string = vcf[irow, :ALT]

        # check if input vcf does not contain it
        if ismissing(ALTs_string)
            push!(PCT_IN_DBs, NaN)
            push!(PROB_PUREs, [NaN for _ in 1:ngroup])
            push!(PROBs, [NaN for _ in 1:ngroup])
            continue
        end

        ALTs = split(ALTs_string, ",")
        # ALTs and DEPTHs should be in same length
        ALTs[1] == "." || pushfirst!(ALTs, ".")

        # DEPTHs will be normalized.
        DEPTHs = parse.(Float64, split(vcf[irow, :AD], ","))
        value_normalize!(DEPTHs)

        # check
        if length(DEPTHs) != length(ALTs)
            @error Pipelines.timestamp() * "BUG" irow DEPTHs ALTs
        end

        PCT_IN_DB = 0.0  # percent of REF/ALT recorded in db_vcf
        PROB_PURE = zeros(ngroup)  # pure-culture probability or sample purity (include ALTs in db_vcf).
        for (i, ALT) in enumerate(ALTs)
            iDEPTH = DEPTHs[i]

            #TODO: filters goes here

            if haskey(ALT2PROBs, ALT)
                PCT_IN_DB += iDEPTH
                PROB_PURE .+= (ALT2PROBs[ALT] .* iDEPTH)
            end
        end
        PROB = PROB_PURE ./ PCT_IN_DB  # probability in groups (ignore ALTs not in db_vcf).

        push!(PCT_IN_DBs, PCT_IN_DB)
        push!(PROB_PUREs, PROB_PURE)
        push!(PROBs, PROB)
    end

    vcf[!, :PCT_IN_DBs] = PCT_IN_DBs
    vcf[!, :PROB_PUREs] = PROB_PUREs
    vcf[!, :PROBs] = PROBs

    vcf
end

"""
    colsums(vec_in_vec::Vector{Vector{T}}) where T <: Number

Sum the pseudo-column of
"""
function colsums(vec_in_vec::Vector{Vector{T}}) where T <: Number
    n_row = length(vec_in_vec)
    n_row == 0 && return Vector()
    n_col = length(vec_in_vec[1])
    n_col == 0 && return Vector()

    map(i_col -> sum([row[i_col] for row in vec_in_vec]), 1:n_col)
end

function colsums(vec_in_vec::Vector{Vector{Float64}}; nan_rm = false)
    n_row = length(vec_in_vec)
    n_row == 0 && return Vector()
    n_col = length(vec_in_vec[1])
    n_col == 0 && return Vector()

    if nan_rm
        res = Float64[0.0 for _ in 1:n_col]
        for row in vec_in_vec
            for (i_col, value) in enumerate(row)
                isnan(value) && continue
                res[i_col] += value
            end
        end
        return res
    else
        map(i_col -> sum([row[i_col] for row in vec_in_vec]), 1:n_col)
    end
end

"""
    hasmissing(vec::Vector)::Bool

Return `true` if `vec` has `missing`.
"""
@inline hasmissing(vec::AbstractArray)::Bool = !isnothing(findfirst(ismissing, vec))

function print_df_result(io::IO, df::DataFrame; title::AbstractString="", suffix::AbstractString="")
    len_title = length.(split(title, "\n")) |> maximum
    if len_title > 120
        len_title = 120
    end

    if len_title != 0
        line = "─" ^ len_title
        println(io, "")
        println(io, line)
        println(io, title)

        show(io, df, allrows=true, allcols=true, summary=false)
        println(io, "")

        length(suffix) > 0 && println(io, "\n", suffix)
        println(io, line)
    else
        show(io, df, allrows=true, allcols=true, summary=false)
        println(io, "")
        length(suffix) > 0 && println(io, suffix)
    end
    println(io, "")
end

# """
#     classifier_single_sample(input::AbstractString, outprefix::AbstractString, db_vcf_parsed::DataFrame, groups::Vector; input_label::String = ":auto", SNP_coverage_cutoff::Real = 50)

# ## Positional arguments

# - `input::AbstractString`: path of input file.

# - `outprefix::AbstractString`: prefix of output file. `<input>` will be replaced with `input` filename.

# - `db_vcf_parsed::DataFrame`: a `DataFrame` generated by function `parse_group_db_vcf()`.

# - `groups::Vector`: the group names coresponding to the probabilities in `db_vcf_parsed`.

# ## Options

# - `input_label::String`: label of input. If `":auto"`, it will set to ClassifiedResult/BasenameOfInput.

# - `SNP_coverage_cutoff::Real`: if the SNP coverage for input sample is less than it, the classification will become `Negative or Low Covered SNP (<NUM)`. It is interpreted as percentage if <= 1, as count if > 1.
# """
# function classifier_single_sample(input::AbstractString, outprefix::AbstractString,
#     db_vcf_parsed::DataFrame, groups::Vector;
#     input_label::String = ":auto", SNP_coverage_cutoff::Real = 50
# )
#     input_vcf_all = vcf_load(input)
#     if isnothing(input_vcf_all)
#         return (nothing, nothing, nothing, nothing, nothing, nothing, nothing)
#     end

#     # compute group probability for input sample
#     vcf = compute_input_probabilities(input_vcf_all, db_vcf_parsed, groups)

#     group_score_pure = colsums(vcf.PROB_PUREs, nan_rm=true)
#     group_score = colsums(vcf.PROBs, nan_rm=true)

#     count_covered_SNP = count(x -> findfirst(isnan, x) === nothing, vcf.PROBs)
#     count_all_SNP = length(vcf.PROBs)
#     percent_covered = count_covered_SNP/count_all_SNP

#     result = DataFrame(
#         GROUP = groups,
#         PURE_SCORE = group_score_pure,
#         SCORE = group_score
#     )

#     if SNP_coverage_cutoff > 1
#         # regard input cutoff as count, transform to percent first
#         percent_SNP_coverage_cutoff = SNP_coverage_cutoff/count_all_SNP
#         negative_classification = "Negative or Low Covered SNP (<$(round(Int, SNP_coverage_cutoff)))"
#     else
#         percent_SNP_coverage_cutoff = SNP_coverage_cutoff
#         negative_classification = "Negative or Low Covered SNP (<$(round(Int, SNP_coverage_cutoff * 100))%)"
#     end

#     max_score, max_index = findmax(group_score)
#     classify_to = if percent_covered >= percent_SNP_coverage_cutoff
#         groups[max_index]
#     else
#         negative_classification
#     end

#     # check labels: if == :auto, generate ClassifiedResult/BasenameOfInput
#     label = input_label == ":auto" ?
#         classify_to * "/" * basename(input) :
#         input_label

#     # generate vcf with coverage
#     covered_vcf = @linq vcf |> where(:ALT .!== missing)
#     covered_vcf_mlst = parsed_db_vcf_to_mlst(covered_vcf, groups; remain_alt2probs=false)
#     covered_vcf_mlst.ALT = covered_vcf.ALT
#     covered_vcf_mlst = covered_vcf_mlst[!, [1,2,3,end, 4:end-1...]]  # move ALT column after REF

#     ### output
#     outprefix = replace(outprefix, "<input>" => input)

#     # out result
#     out_result_path = "$outprefix.result.tsv"
#     CSV.write(out_result_path, result, delim='\t')
#     print_df_result(
#         stdout, result;
#         title="""
# Classification Result
# $input

#   Covered SNPs = $count_covered_SNP
#   All SNPs     = $count_all_SNP
#   Classification  = $classify_to""",
#         suffix="Saved to $out_result_path"
#     )

#     # out mlst
#     out_mlst_path = "$outprefix.MLST.tsv"
#     CSV.write(out_mlst_path, covered_vcf_mlst, delim='\t')
#     @info Pipelines.timestamp() * "MLST variation table saved to $out_mlst_path"

#     # out vcf with probability
#     version_classifier = v"0.0.2"
#     out_jld2_path = "$outprefix.vars.jld2"
#     @save out_jld2_path input label classify_to vcf result groups version_classifier
#     @info Pipelines.timestamp() * "Raw data (input label classify_to vcf result groups version_classifier) saved to $out_jld2_path"


#     return (input, label, classify_to, vcf, result, count_covered_SNP, count_all_SNP)
# end


function parsed_db_vcf_to_mlst!(db_vcf_parsed::DataFrame, groups::Vector; remain_alt2probs::Bool=true)
    # add groups to db
    offset = 4  # CHROM│ POS│ REF│ ALT2PROBs
    select!(db_vcf_parsed, [:CHROM, :POS, :REF, :ALT2PROBs])
    for i in groups
        if i in ("CHROM", "POS", "REF", "ALT2PROBs")
            i += "1"  # prevent duplicated name
        end
        db_vcf_parsed[!, Symbol(i)] .= ""
    end

    ngroup = length(groups)
    for row in eachrow(db_vcf_parsed)
        for pair in collect(row.ALT2PROBs)
            alt = pair.first
            probs = pair.second
            for i in 1:ngroup
                prob = probs[i]
                if prob > 0.0
                    res_i = alt * "(" * string(round(Int, prob * 100)) * "%)"
                    if row[offset+i] == ""
                        row[offset+i] = res_i
                    else
                        row[offset+i] *= " / " * res_i
                    end
                end
            end
        end
    end
    group_perm = sortperm(groups) .+ offset
    if remain_alt2probs
        select!(db_vcf_parsed, [1,2,3,4, group_perm...])
    else
        select!(db_vcf_parsed, [1,2,3, group_perm...])
    end
end

function overlap(db_vcf_parsed::DataFrame, record::GFF3.Record; variants_only::Bool=true, remain_alt2probs::Bool=false)
    res = @linq db_vcf_parsed |>
        where(:CHROM .== GFF3.seqid(record),
              :POS .>= GFF3.seqstart(record),
              :POS .<= GFF3.seqend(record))

    # relative position
    offset = minimum(res[!,:POS]) - 1
    res.OFFSET = res[!,:POS] .- offset

    if variants_only
        res = @linq res |> where(length.(:ALT2PROBs) .> 1)
    end

    if remain_alt2probs
        select!(res, [ncol(res), 1, 2, 3, 4, (5:ncol(res)-1)...])
    else
        select!(res, [ncol(res), 1, 2, 3, (5:ncol(res)-1)...])
    end
end
